---
title: "R, tidyverse, googledrive, and github pages"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  html_notebook:
    toc: yes
    toc_float: yes
    toc_depth: 2
    collapsed: no
    theme: flatly
---

# Intro

There is kind of a lot going on in this show-and-tell so I'm taking a moment up-front to give some why behind the what. 

The first thing I'm providing here is a quick intro to my GitHub Pages Blog. The reason I'm providing this background is that I'm writing this tutorial in Rmarkdown. After we go through the ```googledrive``` and ```AWS Database``` stuff, I'm going to render (or try at least) this to .md or .html and push it up my blog for distribution.

The second item on the agenda is the [tidyverse::googledrive package](https://googledrive.tidyverse.org/). I don't have a ton of interesting stuff to say about this package. I'm providing some code that works pretty well for me. These code samples come with the following caveat: so far everything about the ```tidyverse::googledrive``` package has worked for me without incident. I may not be super helpful in the troubleshooting arena as I haven't really had to troubleshoot anything yet.   

The third topic I'm touching on here is importing data from a MySQL database running on Amazon cloud. I have a few other pretty involved blog posts on this topic so I'm really just planning to scratch the surface a little on this one. The main reason I'm doing this is that I think there are some limitations to the Google Drive workflow and I wanted to illustrate one pretty simple alternative.

# Packages and Dependencies

```{r}
library(googledrive)
library(stringr) # some string functions for cleaning EIDL data
library(dplyr)
library(DBI) # for some database interface methods
library(here) # manage directories
library(DT) # for pretty Rmarkdown tables
```

# GitHub Blog

I've written what I think is a pretty digestible walk-through on getting started using GitHub pages to host a research blog. This walk-through lives in the most unlikely of places, my research blog: [Blogging on GitHub Pages with Jekyll](https://aaronmams.github.io/test/).

I don't want to (and I'm not really qualified to anyway) get hung-up on the particulars of webhosting with GitHub pages. What I want to emphasize at this stage is that, if you want to get a simple website/blog up and running, git/GitHub make it pretty easy. I did it in like 2 hours (possibly less): 

1. forked an existing open-source GitHub repository that happened to be a Jekyll blog
2. changed the name of that repo to ```aaronmams.github.io```

By naming the repository ```github-username.github.io``` GitHub knows to associate the content of that repo with the GitHub pages domain they automagically allocate to my GitHub account.

First point of order: you don't have to do things the way I did them. You can build a website "from the ground up" if you want and put it up on GitHub pages by putting the source files in the GitHub repository called ```github-username.github.io```. [Emily Markowitz](https://emilyhmarkowitz.github.io/emilyhmarkowitz/) has a really cool personal website that she runs on GitHub pages. 

Second point of order: the static site generator Jekyll plays no role in the day-to-day maintenance of my research blog. I manage new posts through R Studio & GitHub by writing new posts in Rmarkdown, rendering them to markdown (.md), and pushing them to the ```_posts/``` sub-directory of the repository. The only Jekyll-centric element of my blog that really matters is that new posts must be named according to the convention: ```2020-10-27-cool blog title```. This is just how Jekyll demands that posts be named in order to recognize them as content to be rendered to the site. 


# Google Drive {.tabset .tabset-fade .tabset-pills}

## Authentication/Connectivity

Probably the most important piece of the ```tidyverse::googledrive``` pipeline is the authentication process. Unfortunately, I don't have much to say about this. Basically, the ```googledrive``` package pretty much did all the work for me. 

Because it's difficult to illustrate this authentication process "on-the-fly", I did a quick 5 min. screen recording that's available on YouTube, [R-googledrive-authentication](https://youtu.be/lvP_Sf8DrJM).

```{r}
drive_auth()
drive_user()
```

## Find Resources

Using the ```drive_find()``` method we can explore Google Drive resources available in our session:

```{r}
datatable(drive_find(n_max=10))
```

One potentially useful way to filter results from ```drive_find()``` is to search for resources of a particular type. Here, I'll look for google sheet file types. Other arguments that can be provided include 

* "csv"
* application/pdf

```{r}
datatable(drive_find(type="spreadsheet",n_max=10))

```


The method ```drive_get()``` pulls down a lot of meta-type info on available resources. 

```{r}
example <- drive_get("R Workshop Video Links")
example
```

Note that the field ```drive_resource``` nests a rather large list in a tibble column.  

```{r}
example$drive_resource
```
I don't currently have a need for most of this info...but I can imagine that some people may want to look-up google drive resources by characteristics or properties. For example, I can see where it might be valuable to know who the last person to modify a file was:

```{r}
example$drive_resource[[1]]$lastModifyingUser
```

## File Downloads

The ```drive_find()``` and ```drive_get()``` methods are nice discovery tools. If we have actual source data that we would like to access from Google Drive, we need to bring it down into the workspace. For this we can use ```drive_download()```.

I have some data from the Small Business Administration on Covid relief from the Paycheck Protection Program and Economic Injury Disaster Loans Program. These data exist as a collection of .csv files currently housed in google drive. The following will download one of these data files into the ```data/``` subdirectory of my current project.

```{r}
# download the EIDL data from google drive into the local "data" sub-directory
drive_download("Cara-R-Internship/CovidData/EIDLLoans1.csv",path="C:/Users/aaron.mamula/Desktop/R-Projects/PPP-EIDL-Analysis/data/EIDLLoans1.csv",
               overwrite=T)


```

Something that I found interesting is that the ```drive_download()``` method doesn't actually require the specificity that I provided above. The method will find my ```EIDLLoans1.csv``` file whether I provide it nesting folder names or not:

```{r}
drive_download("PPP Data 150k plus 080820.csv",path="C:/Users/aaron.mamula/Desktop/R-Projects/PPP-EIDL-Analysis/data/ppp.csv",
               overwrite=T)
```

The ```drive_download()``` function made a local copy of the .csv file in my ```data/``` sub-directory. To access these data in the R Workspace we just use the familiar ```read.csv()``` method:

```{r}
df <- read.csv("C:/Users/aaron.mamula/Desktop/R-Projects/PPP-EIDL-Analysis/data/ppp.csv")
str(df)
```

```{r}
#clean city names & clean loan range field
df <- df %>% mutate(city.name=toupper(trimws(City)),
                    state = toupper(trimws(State)),
                    LoanRange = trimws(substr(LoanRange,2,nchar(LoanRange)))) 

# find Santa Cruz County Businesses
ppp.sc <- df %>% filter(Zip %in% c(95001,95010,05018,95017,95019,95041,95060,
                                   95062,95065,96064,95066,95073,95003,95005,95007))
```

```{r}
# Organize by JobsReported
ppp.sc <- ppp.sc %>% arrange(-JobsReported) 
datatable(ppp.sc %>% select(LoanRange,BusinessName,City,JobsReported,DateApproved))
```


What if one would like to import data from a .csv in google drive WITHOUT downloading a local version of the .csv? 

Unfortunately, I don't have a great answer for that. Here are a couple leads for you tho:

1. There is a [googlesheets](https://github.com/tidyverse/googlesheets4) package that is part of the tidyverse. So if you're ok having the data in a google sheet rather than a .csv this could be a good option.

2. There is probably a way to use ```read.csv()``` directly on the url. I haven't had any success in this arena yet but it could just be that I'm not understanding what url string I should be using. [This Stack Overflow question](https://stackoverflow.com/questions/33135060/read-csv-file-hosted-on-google-drive) claims to have cracked the case. 


# AWS {.tabset .tabset-fade .tabset-pills}

This might seem like kind of a tangent but I think it's related enough to the previous ```googledrive``` section to warrant inclusion. Here's why: Google Drive is a nice, easy-to-use cloud platform for data storage. AWS is another cloud-based alternative that requires a little more set-up effort but results (I think) in a cleaner workflow.

I have a series of blog posts where I tried to do a reasonably thorough reporting on a self-guided AWS expedition I took a while back:

1. [Connect to Amazon RDS w/MySQL Workbench](https://aaronmams.github.io/Connect-to-Amazon-RDS-DB-with-MySQL-Workbench/)
2. [AWS Custom VPCs](https://aaronmams.github.io/AWS-MySQL-DB-in-a-Custom-VPC/)
3. [Using a Virtual EC2 Server to Connect to RDS](https://aaronmams.github.io/Cloud-computing-with-Python,-MySQL-in-AWS/)
4. [Using Python Script to Connct to AWS RDS](https://aaronmams.github.io/Connecting-python-to-AWS-RDS-DB-in-the-cloud/)

## DB Backstory

I got a bunch of needle logs from the City of Santa Cruz. These are reports that city workers fill out whenever they pick up a used needle from a public space. They write down an approximate address/description and how many needles were encountered. I received these data as hard copies and jammed them into .csv files. Then I looked up the addresses as best I could on google maps and created a look-up table with lat/long coordinates for each location.

The database currently has two tables:

1. ```needle_events``` which contains the individual needle encounters
2. ```location_geo``` which contains the lat/long coordinates for each location description.

## DB Set-up

[This a pretty detailed log of my first experience setting up a relational database on AWS](https://aaronmams.github.io/Connect-to-Amazon-RDS-DB-with-MySQL-Workbench/). In the article I also link to a number of other tutorials that can be used to set up a simple relational database on Amazon's RDS service. 

This step is pretty much just toggling through and selecting from a menu of options provided to you by Amazon.


## DB Connection

From here it's totally feasible to connect to your database through R. I like to preface my R connection with a connection via some other database client. Since the DB I'm using here is a MySQL database, I used MySQL Workbench (a free MySQL client) to establish my initial connection. This has the added benefit of giving me a nice pointy-clicky interface with which to write new observations to database tables by pushing .csv files up.

The R connection is really simple (mostly because I don't have much security fencing on this database): just need a database endpoint and some access credentials:

```{r}
cn <- dbConnect(drv      = RMySQL::MySQL(), 
                username = "admin", 
                password = "nmfssocialscience", 
                host     = "mams-teaching-public.c65i4tmttvql.us-west-1.rds.amazonaws.com", 
                port     = 3306, 
                dbname   = "needle_waste")

```

## Analysis

Once the database connection is established, it is pretty straightforward to import data and start doing stuff with it.

The ```dbGetQuery()``` method from the ```DBI``` package allows SQL syntax to be passed to the database engine and returns results to the R workspace. Here, I'm joining two tables from the database 

```{r}
df.joined <- dbGetQuery(cn, "SELECT * FROM needle_events inner join locations_geo on needle_events.LOCATION_RECORDED=locations_geo.LOCATION_RECORDED")
datatable(head(df.joined))
```

# Unsolicited Commentary

I think a lot of us were moved (probably to varying degrees) by the Richard McElreath video that Roy recently distributed. The portion of the talk that resonated most with me with was the emphasis placed on proper data management/data curation practices. Basically, it's hard for research to be reproducible if the data aren't shareable. And it's hard for data to be shareable if they exist as a potpourri of flat files on somebody's hard-drive. 

I'd like to think that what I presented here could kind of be marketed as a "starter kit" for working with data in "the cloud"...which is kind of an important skill for those interested in reproducable research.   

Consider the following popular workflow for managing research projects with multiple contributors:

Github fork --> git clone --> R Studio code --> git commit --> git push/pull

This is good for collaborative analysis but GitHub isn't a great place for source data. So if the source data are being periodically updated, you need a data distribution platform that's accessible to all project collaborators.

Google Drive and Amazon Web Services are two relatively low-cost cloud data storage platforms. This makes them nice options for practicing collaborative data management. 






